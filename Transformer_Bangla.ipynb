{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import math\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "fz6177-py2sO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "    d_k = q.size()[-1]\n",
        "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scaled = scaled.permute(1, 0, 2, 3) + mask\n",
        "        scaled = scaled.permute(1, 0, 2, 3)\n",
        "    attention = F.softmax(scaled, dim=-1)\n",
        "    values = torch.matmul(attention, v)\n",
        "    return values, attention"
      ],
      "metadata": {
        "id": "TitXS5tGy7cX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_sequence_length):\n",
        "        super().__init__()\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self):\n",
        "        even_i = torch.arange(0, self.d_model, 2).float()\n",
        "        denominator = torch.pow(10000, even_i/self.d_model)\n",
        "        position = (torch.arange(self.max_sequence_length)\n",
        "                          .reshape(self.max_sequence_length, 1))\n",
        "        even_PE = torch.sin(position / denominator)\n",
        "        odd_PE = torch.cos(position / denominator)\n",
        "        stacked = torch.stack([even_PE, odd_PE], dim=2)\n",
        "        PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
        "        return PE"
      ],
      "metadata": {
        "id": "B5yBaPI5zADb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SentenceEmbedding(nn.Module):\n",
        "    def __init__(self, max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN):\n",
        "        super().__init__()\n",
        "        self.vocab_size = len(language_to_index)\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.embedding = nn.Embedding(self.vocab_size, d_model)\n",
        "        self.language_to_index = language_to_index\n",
        "        self.position_encoder = PositionalEncoding(d_model, max_sequence_length)\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "        self.START_TOKEN = START_TOKEN\n",
        "        self.END_TOKEN = END_TOKEN\n",
        "        self.PADDING_TOKEN = PADDING_TOKEN\n",
        "\n",
        "    def batch_tokenize(self, batch, start_token, end_token):\n",
        "\n",
        "        def tokenize(sentence, start_token, end_token):\n",
        "            sentence_word_indicies = [self.language_to_index[token] for token in list(sentence)]\n",
        "            if start_token:\n",
        "                sentence_word_indicies.insert(0, self.language_to_index[self.START_TOKEN])\n",
        "            if end_token:\n",
        "                sentence_word_indicies.append(self.language_to_index[self.END_TOKEN])\n",
        "            for _ in range(len(sentence_word_indicies), self.max_sequence_length):\n",
        "                sentence_word_indicies.append(self.language_to_index[self.PADDING_TOKEN])\n",
        "            return torch.tensor(sentence_word_indicies)\n",
        "\n",
        "        tokenized = []\n",
        "        for sentence_num in range(len(batch)):\n",
        "           tokenized.append( tokenize(batch[sentence_num], start_token, end_token) )\n",
        "        tokenized = torch.stack(tokenized)\n",
        "        return tokenized.to(get_device())\n",
        "\n",
        "    def forward(self, x, start_token, end_token): # sentence\n",
        "        x = self.batch_tokenize(x, start_token, end_token)\n",
        "        x = self.embedding(x)\n",
        "        pos = self.position_encoder().to(get_device())\n",
        "        x = self.dropout(x + pos)\n",
        "        return x"
      ],
      "metadata": {
        "id": "6vzqWNmmzDXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.qkv_layer = nn.Linear(d_model , 3 * d_model)\n",
        "        self.linear_layer = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        batch_size, sequence_length, d_model = x.size()\n",
        "        qkv = self.qkv_layer(x)\n",
        "        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3 * self.head_dim)\n",
        "        qkv = qkv.permute(0, 2, 1, 3)\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "        values, attention = scaled_dot_product(q, k, v, mask)\n",
        "        values = values.permute(0, 2, 1, 3).reshape(batch_size, sequence_length, self.num_heads * self.head_dim)\n",
        "        out = self.linear_layer(values)\n",
        "        return out"
      ],
      "metadata": {
        "id": "BQ2pEKIMzLlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "    def __init__(self, parameters_shape, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.parameters_shape=parameters_shape\n",
        "        self.eps=eps\n",
        "        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n",
        "        self.beta =  nn.Parameter(torch.zeros(parameters_shape))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        dims = [-(i + 1) for i in range(len(self.parameters_shape))]\n",
        "        mean = inputs.mean(dim=dims, keepdim=True)\n",
        "        var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True)\n",
        "        std = (var + self.eps).sqrt()\n",
        "        y = (inputs - mean) / std\n",
        "        out = self.gamma * y + self.beta\n",
        "        return out"
      ],
      "metadata": {
        "id": "8RxAfi1KzM-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, hidden)\n",
        "        self.linear2 = nn.Linear(hidden, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "TxNqVdibzTBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "        self.norm1 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
        "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
        "        self.norm2 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x, self_attention_mask):\n",
        "        residual_x = x.clone()\n",
        "        x = self.attention(x, mask=self_attention_mask)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.norm1(x + residual_x)\n",
        "        residual_x = x.clone()\n",
        "        x = self.ffn(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.norm2(x + residual_x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "0dBuitB4zUXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SequentialEncoder(nn.Sequential):\n",
        "    def forward(self, *inputs):\n",
        "        x, self_attention_mask  = inputs\n",
        "        for module in self._modules.values():\n",
        "            x = module(x, self_attention_mask)\n",
        "        return x"
      ],
      "metadata": {
        "id": "mQ-XG93wzaGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 ffn_hidden,\n",
        "                 num_heads,\n",
        "                 drop_prob,\n",
        "                 num_layers,\n",
        "                 max_sequence_length,\n",
        "                 language_to_index,\n",
        "                 START_TOKEN,\n",
        "                 END_TOKEN,\n",
        "                 PADDING_TOKEN):\n",
        "        super().__init__()\n",
        "        self.sentence_embedding = SentenceEmbedding(max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "        self.layers = SequentialEncoder(*[EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob)\n",
        "                                      for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x, self_attention_mask, start_token, end_token):\n",
        "        x = self.sentence_embedding(x, start_token, end_token)\n",
        "        x = self.layers(x, self_attention_mask)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "kk49JbI5zcu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadCrossAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.kv_layer = nn.Linear(d_model , 2 * d_model)\n",
        "        self.q_layer = nn.Linear(d_model , d_model)\n",
        "        self.linear_layer = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x, y, mask):\n",
        "        batch_size, sequence_length, d_model = x.size() # in practice, this is the same for both languages...so we can technically combine with normal attention\n",
        "        kv = self.kv_layer(x)\n",
        "        q = self.q_layer(y)\n",
        "        kv = kv.reshape(batch_size, sequence_length, self.num_heads, 2 * self.head_dim)\n",
        "        q = q.reshape(batch_size, sequence_length, self.num_heads, self.head_dim)\n",
        "        kv = kv.permute(0, 2, 1, 3)\n",
        "        q = q.permute(0, 2, 1, 3)\n",
        "        k, v = kv.chunk(2, dim=-1)\n",
        "        values, attention = scaled_dot_product(q, k, v, mask) # We don't need the mask for cross attention, removing in outer function!\n",
        "        values = values.permute(0, 2, 1, 3).reshape(batch_size, sequence_length, d_model)\n",
        "        out = self.linear_layer(values)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "G1WRPJ1qzhED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "        self.layer_norm1 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "        self.encoder_decoder_attention = MultiHeadCrossAttention(d_model=d_model, num_heads=num_heads)\n",
        "        self.layer_norm2 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
        "        self.layer_norm3 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout3 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x, y, self_attention_mask, cross_attention_mask):\n",
        "        _y = y.clone()\n",
        "        y = self.self_attention(y, mask=self_attention_mask)\n",
        "        y = self.dropout1(y)\n",
        "        y = self.layer_norm1(y + _y)\n",
        "\n",
        "        _y = y.clone()\n",
        "        y = self.encoder_decoder_attention(x, y, mask=cross_attention_mask)\n",
        "        y = self.dropout2(y)\n",
        "        y = self.layer_norm2(y + _y)\n",
        "\n",
        "        _y = y.clone()\n",
        "        y = self.ffn(y)\n",
        "        y = self.dropout3(y)\n",
        "        y = self.layer_norm3(y + _y)\n",
        "        return y\n"
      ],
      "metadata": {
        "id": "I1tqDFXfzh4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SequentialDecoder(nn.Sequential):\n",
        "    def forward(self, *inputs):\n",
        "        x, y, self_attention_mask, cross_attention_mask = inputs\n",
        "        for module in self._modules.values():\n",
        "            y = module(x, y, self_attention_mask, cross_attention_mask)\n",
        "        return y\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 ffn_hidden,\n",
        "                 num_heads,\n",
        "                 drop_prob,\n",
        "                 num_layers,\n",
        "                 max_sequence_length,\n",
        "                 language_to_index,\n",
        "                 START_TOKEN,\n",
        "                 END_TOKEN,\n",
        "                 PADDING_TOKEN):\n",
        "        super().__init__()\n",
        "        self.sentence_embedding = SentenceEmbedding(max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "        self.layers = SequentialDecoder(*[DecoderLayer(d_model, ffn_hidden, num_heads, drop_prob) for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x, y, self_attention_mask, cross_attention_mask, start_token, end_token):\n",
        "        y = self.sentence_embedding(y, start_token, end_token)\n",
        "        y = self.layers(x, y, self_attention_mask, cross_attention_mask)\n",
        "        return y"
      ],
      "metadata": {
        "id": "l6Qrdmjrzk3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZAsokfIubhw"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                d_model,\n",
        "                ffn_hidden,\n",
        "                num_heads,\n",
        "                drop_prob,\n",
        "                num_layers,\n",
        "                max_sequence_length,\n",
        "                kn_vocab_size,\n",
        "                english_to_index,\n",
        "                bangla_to_index,\n",
        "                START_TOKEN,\n",
        "                END_TOKEN,\n",
        "                PADDING_TOKEN\n",
        "                ):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, english_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "        self.decoder = Decoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, bangla_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "        self.linear = nn.Linear(d_model, kn_vocab_size)\n",
        "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "    def forward(self,\n",
        "                x,\n",
        "                y,\n",
        "                encoder_self_attention_mask=None,\n",
        "                decoder_self_attention_mask=None,\n",
        "                decoder_cross_attention_mask=None,\n",
        "                enc_start_token=False,\n",
        "                enc_end_token=False,\n",
        "                dec_start_token=True,\n",
        "                dec_end_token=False):\n",
        "        x = self.encoder(x, encoder_self_attention_mask, start_token=enc_start_token, end_token=enc_end_token)\n",
        "        out = self.decoder(x, y, decoder_self_attention_mask, decoder_cross_attention_mask, start_token=dec_start_token, end_token=dec_end_token)\n",
        "        out = self.linear(out)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "english_file = '/content/drive/MyDrive/Colab Notebooks/AdvanceDeepLearning/Dataset/original_corpus.en'\n",
        "bangla_file = '/content/drive/MyDrive/Colab Notebooks/AdvanceDeepLearning/Dataset/original_corpus.bn'\n",
        "START_TOKEN = '<START>'\n",
        "PADDING_TOKEN = '<PADDING>'\n",
        "END_TOKEN = '<END>'\n",
        "\n",
        "bangla_vocabulary = [START_TOKEN, ' ','|', '!', '¡','\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/',\n",
        "                      '০', '১', '২', '৩', '৪', '৫', '৬', '৭', '৮', '৯', ':', '<', '=', '>', '?', 'ˌ',\n",
        "                      'অ', 'আ', 'ই', 'ঈ', 'উ', 'ঊ', 'ঋ', 'ৠ',\n",
        "                      'এ', 'ঐ', 'ও', 'ক', 'খ', 'গ', 'ঘ', 'ঙ', 'চ', 'ছ', 'জ', 'ঝ', 'ঞ', 'ট', 'ঠ',\n",
        "                      'ড', 'ঢ', 'ণ', 'ত', 'থ',\n",
        "                      'দ', 'ধ', 'ন', 'প', 'ফ',\n",
        "                      'ব', 'ভ', 'ম', 'য', 'শ',\n",
        "                      'ষ', 'স', 'হ', 'ড়', 'ঢ়',\n",
        "                      'য়', 'ৎ', 'া','ি', 'ু', 'ৃ',\n",
        "                      'ৢ', 'ে', 'ো', 'ী', 'ূ', 'ৈ', 'ৌ', '‍ঁ', 'ং', 'ঃ',\n",
        "                      '‍্', '‍্য', '‍‍্র', 'ক্র', 'চ্ছ্ব', 'ক্ষ', 'ন্ট', 'দ্ধ', 'ন্ধ', 'ণ্ঠ', 'ভ্র', 'স্ত', 'হ্ন', 'ন্ত', 'ত্ব', 'ন্ব', 'ক্স', 'ম্ব', 'জ্ঞ', 'ষ্ট', 'ন্ত্র', 'ক্ত',\n",
        "                      'ষ্ঠ', 'ন্ন', 'ত্ন', 'দ্দ', 'প্ত', 'হ্ম', 'ব্জ', 'ন্দ', 'ন্ড', 'ব্দ','ষ্ণ', PADDING_TOKEN, END_TOKEN]\n",
        "\n",
        "english_vocabulary = [START_TOKEN, ' ', '!', '¡','\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/',\n",
        "                        '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
        "                        ':', '<', '=', '>', '?', '@',\n",
        "                        '[', '\\\\', ']', '^', '_', '`',\n",
        "                        'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
        "                        'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x',\n",
        "                        'y', 'z',\n",
        "                        '{', '|', '}', '~', PADDING_TOKEN, END_TOKEN]\n"
      ],
      "metadata": {
        "id": "c36GoFtkvTVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_bangla = {k:v for k,v in enumerate(bangla_vocabulary)}\n",
        "bangla_to_index = {v:k for k,v in enumerate(bangla_vocabulary)}\n",
        "index_to_english = {k:v for k,v in enumerate(english_vocabulary)}\n",
        "english_to_index = {v:k for k,v in enumerate(english_vocabulary)}"
      ],
      "metadata": {
        "id": "jJZQzO30vp50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(english_file, 'r') as file:\n",
        "    english_sentences = file.readlines()\n",
        "with open(bangla_file, 'r') as file:\n",
        "    bangla_sentences = file.readlines()\n",
        "\n",
        "# Limit Number of sentences\n",
        "TOTAL_SENTENCES = 100000\n",
        "english_sentences = english_sentences[:TOTAL_SENTENCES]\n",
        "bangla_sentences = bangla_sentences[:TOTAL_SENTENCES]\n",
        "english_sentences = [sentence.rstrip('\\n').lower() for sentence in english_sentences]\n",
        "bangla_sentences = [sentence.rstrip('\\n') for sentence in bangla_sentences]"
      ],
      "metadata": {
        "id": "ldrVjt-LvryZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english_sentences[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYxUuBLivwwh",
        "outputId": "52c11069-ad1d-46cd-920b-165cdd093ab5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['he turned to look langdon in the eye.',\n",
              " 'better o . guaranteeing that every individual will be free to do as he wishes',\n",
              " '\"what do you say i tell you over dinner.\"',\n",
              " 'when i was just about to say good-night to the assembly and to leave, a man came after me quickly and introduced himself.',\n",
              " 'let me pass.\"',\n",
              " 'now the camerlegno turned and addressed the remaining guards.',\n",
              " '\"where is their taxi now?',\n",
              " \"evidence for religion, commerce and social stratification. most researchers believe that these unprecedented accomplishments were the product of a revolution in sapiens' cognitive abilities.\",\n",
              " 'for decades, palaeontologists and zooarchaeologists - people who search for and study animal remains - have been combing the plains and mountains of the americas in search of the fossilised bones of ancient camels and the petri ed faeces of giant ground sloths.',\n",
              " 'it represents a kind of school for adults.']"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bangla_sentences[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_uC4hyVvzhg",
        "outputId": "5d9d36d5-81b6-4f64-aabe-3578f52dc8f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['সে ঘুরে ল্যাংডনের চোখের দিকে তাকায়।',\n",
              " 'সব মানুষের মধ্যে সমতা আনতে হলে কারও না কারও স্বাধীনতায় হস্তক্ষেপ করতেই হবে।',\n",
              " 'যাই জিজ্ঞেস কর না কেন, সব প্রশ্নের জবাব দিব ডিনারের পর।',\n",
              " 'ঠিক যখন আমি সভাকে শুভরাত জানিয়ে বিদায় নিতে উদ্যত, একজন লোক সত্বর আমার কাছে এসে নিজের পরিচয় দেয়।',\n",
              " 'যেতে দিন!',\n",
              " 'এবার ক্যামারলেনগো ঘুরে দাঁড়াল আবার, তাকাল অন্য সৈনিকদের দিকে, জোয়ানগণ, আমি আর কোন প্রাণঘাতি ঘটনা দেখতে চাই না এই সন্ধ্যায়।',\n",
              " 'তাদের ট্যাক্সি এখন কোথায়?',\n",
              " 'বেশিরভাগ গবেষকই মনে করেন যে, এতসব গুরুত্বপূর্ণ আবিষ্কারের পেছনে নিশ্চয়ই সেপিয়েন্সদের বুদ্ধিবৃত্তিক দক্ষতার কোনো পরিবর্তন দায়ী।',\n",
              " 'দশকের পর দশক ধরে এসব প্রাণীর জীবাশ্ম আর দেহাবশেষের খোঁজে দুই আমেরিকার পাহাড় ও সমতলে চষে বেড়াচ্ছেন বিশেষজ্ঞরা। যখনই তাঁরা কোনো কিছু খুঁজে পাচ্ছেন পরম যত্নে সেগুলো পাঠিয়ে দিচ্ছেন গবেষণাগারে।',\n",
              " 'এটা প্রাপ্ত বয়স্কদের জন্য এক রকমের স্কুলও বলা চলে।']"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "PERCENTILE = 97\n",
        "print( f\"{PERCENTILE}th percentile length Bangla: {np.percentile([len(x) for x in bangla_sentences], PERCENTILE)}\" )\n",
        "print( f\"{PERCENTILE}th percentile length English: {np.percentile([len(x) for x in english_sentences], PERCENTILE)}\" )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YGh-5d5v18l",
        "outputId": "2cf53b4d-0be8-416b-e05b-8a5aacff1258"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "97th percentile length Bangla: 207.0\n",
            "97th percentile length English: 228.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_sequence_length = 250\n",
        "\n",
        "def is_valid_tokens(sentence, vocab):\n",
        "    for token in list(set(sentence)):\n",
        "        if token not in vocab:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def is_valid_length(sentence, max_sequence_length):\n",
        "    return len(list(sentence)) < (max_sequence_length - 1) # need to re-add the end token so leaving 1 space\n",
        "\n",
        "valid_sentence_indicies = []\n",
        "for index in range(len(bangla_sentences)):\n",
        "    bangla_sentence, english_sentence = bangla_sentences[index], english_sentences[index]\n",
        "    if is_valid_length(bangla_sentence, max_sequence_length) \\\n",
        "      and is_valid_length(english_sentence, max_sequence_length) \\\n",
        "      and is_valid_tokens(bangla_sentence, bangla_vocabulary):\n",
        "        valid_sentence_indicies.append(index)\n",
        "\n",
        "print(f\"Number of sentences: {len(bangla_sentences)}\")\n",
        "print(f\"Number of valid sentences: {len(valid_sentence_indicies)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCWDOGSQv6tj",
        "outputId": "7bb2a6a4-5944-4dc3-b0a9-f83f1febd494"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sentences: 100000\n",
            "Number of valid sentences: 945\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bangla_sentences = [bangla_sentences[i] for i in valid_sentence_indicies]\n",
        "english_sentences = [english_sentences[i] for i in valid_sentence_indicies]"
      ],
      "metadata": {
        "id": "HKLjXxG7v-Su"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bangla_sentences[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbWGlJDdwASD",
        "outputId": "30e12d47-6447-40b2-c167-e984e2bb7b45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['যেতে দিন!',\n",
              " 'কীভাবে?',\n",
              " 'দেখ!',\n",
              " 'তিন...',\n",
              " 'টেনশন?',\n",
              " 'কখনও!',\n",
              " 'তাই নাকি?',\n",
              " 'হুম!',\n",
              " 'পিশাচ!',\n",
              " 'দশ মিনিট?']"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "d_model = 512\n",
        "batch_size = 64\n",
        "ffn_hidden = 2048\n",
        "num_heads = 8\n",
        "drop_prob = 0.1\n",
        "num_layers = 5\n",
        "max_sequence_length = 250\n",
        "bn_vocab_size = len(bangla_vocabulary)\n",
        "\n",
        "transformer = Transformer(d_model,\n",
        "                          ffn_hidden,\n",
        "                          num_heads,\n",
        "                          drop_prob,\n",
        "                          num_layers,\n",
        "                          max_sequence_length,\n",
        "                          bn_vocab_size,\n",
        "                          english_to_index,\n",
        "                          bangla_to_index,\n",
        "                          START_TOKEN,\n",
        "                          END_TOKEN,\n",
        "                          PADDING_TOKEN)"
      ],
      "metadata": {
        "id": "V0-o75O5wDWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OjEFvCHwGIc",
        "outputId": "fa02f894-24ad-4372-c4b2-c162d14ba439"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (encoder): Encoder(\n",
              "    (sentence_embedding): SentenceEmbedding(\n",
              "      (embedding): Embedding(72, 512)\n",
              "      (position_encoder): PositionalEncoding()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (layers): SequentialEncoder(\n",
              "      (0): EncoderLayer(\n",
              "        (attention): MultiHeadAttention(\n",
              "          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n",
              "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (norm1): LayerNormalization()\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (ffn): PositionwiseFeedForward(\n",
              "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (norm2): LayerNormalization()\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): EncoderLayer(\n",
              "        (attention): MultiHeadAttention(\n",
              "          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n",
              "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (norm1): LayerNormalization()\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (ffn): PositionwiseFeedForward(\n",
              "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (norm2): LayerNormalization()\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): EncoderLayer(\n",
              "        (attention): MultiHeadAttention(\n",
              "          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n",
              "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (norm1): LayerNormalization()\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (ffn): PositionwiseFeedForward(\n",
              "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (norm2): LayerNormalization()\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (3): EncoderLayer(\n",
              "        (attention): MultiHeadAttention(\n",
              "          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n",
              "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (norm1): LayerNormalization()\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (ffn): PositionwiseFeedForward(\n",
              "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (norm2): LayerNormalization()\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (4): EncoderLayer(\n",
              "        (attention): MultiHeadAttention(\n",
              "          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n",
              "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (norm1): LayerNormalization()\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (ffn): PositionwiseFeedForward(\n",
              "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (norm2): LayerNormalization()\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (sentence_embedding): SentenceEmbedding(\n",
              "      (embedding): Embedding(129, 512)\n",
              "      (position_encoder): PositionalEncoding()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (layers): SequentialDecoder(\n",
              "      (0): DecoderLayer(\n",
              "        (self_attention): MultiHeadAttention(\n",
              "          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n",
              "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (layer_norm1): LayerNormalization()\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (encoder_decoder_attention): MultiHeadCrossAttention(\n",
              "          (kv_layer): Linear(in_features=512, out_features=1024, bias=True)\n",
              "          (q_layer): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (layer_norm2): LayerNormalization()\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        (ffn): PositionwiseFeedForward(\n",
              "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (layer_norm3): LayerNormalization()\n",
              "        (dropout3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): DecoderLayer(\n",
              "        (self_attention): MultiHeadAttention(\n",
              "          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n",
              "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (layer_norm1): LayerNormalization()\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (encoder_decoder_attention): MultiHeadCrossAttention(\n",
              "          (kv_layer): Linear(in_features=512, out_features=1024, bias=True)\n",
              "          (q_layer): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (layer_norm2): LayerNormalization()\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        (ffn): PositionwiseFeedForward(\n",
              "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (layer_norm3): LayerNormalization()\n",
              "        (dropout3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): DecoderLayer(\n",
              "        (self_attention): MultiHeadAttention(\n",
              "          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n",
              "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (layer_norm1): LayerNormalization()\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (encoder_decoder_attention): MultiHeadCrossAttention(\n",
              "          (kv_layer): Linear(in_features=512, out_features=1024, bias=True)\n",
              "          (q_layer): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (layer_norm2): LayerNormalization()\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        (ffn): PositionwiseFeedForward(\n",
              "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (layer_norm3): LayerNormalization()\n",
              "        (dropout3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (3): DecoderLayer(\n",
              "        (self_attention): MultiHeadAttention(\n",
              "          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n",
              "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (layer_norm1): LayerNormalization()\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (encoder_decoder_attention): MultiHeadCrossAttention(\n",
              "          (kv_layer): Linear(in_features=512, out_features=1024, bias=True)\n",
              "          (q_layer): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (layer_norm2): LayerNormalization()\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        (ffn): PositionwiseFeedForward(\n",
              "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (layer_norm3): LayerNormalization()\n",
              "        (dropout3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (4): DecoderLayer(\n",
              "        (self_attention): MultiHeadAttention(\n",
              "          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n",
              "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (layer_norm1): LayerNormalization()\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (encoder_decoder_attention): MultiHeadCrossAttention(\n",
              "          (kv_layer): Linear(in_features=512, out_features=1024, bias=True)\n",
              "          (q_layer): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (layer_norm2): LayerNormalization()\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        (ffn): PositionwiseFeedForward(\n",
              "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (layer_norm3): LayerNormalization()\n",
              "        (dropout3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (linear): Linear(in_features=512, out_features=129, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "\n",
        "    def __init__(self, english_sentences, bangla_sentences):\n",
        "        self.english_sentences = english_sentences\n",
        "        self.bangla_sentences = bangla_sentences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.english_sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.english_sentences[idx], self.bangla_sentences[idx]"
      ],
      "metadata": {
        "id": "02CkqHWnwJm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = TextDataset(english_sentences, bangla_sentences)"
      ],
      "metadata": {
        "id": "MVDZRd3KwJwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCbVW1K1wKAb",
        "outputId": "38a38942-4904-41b5-e46c-a1f438a83ec1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "945"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKTXm_RXwRCb",
        "outputId": "bb1eff3c-2ec9-4d9f-a890-833710995d3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('\"look!\"', 'দেখ!')"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(dataset, batch_size)\n",
        "iterator = iter(train_loader)"
      ],
      "metadata": {
        "id": "WuaMwIl8wTxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch_num, batch in enumerate(iterator):\n",
        "    print(batch)\n",
        "    if batch_num > 3:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvUVsa1SwVtZ",
        "outputId": "e15a5ef8-7d74-4707-b168-297d45102d4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('let me pass.\"', 'but ... how?\"', '\"look!\"', 'three ...', '\"tense?\"', 'ever!\"', '\"really?', '\"hum!', '\"a fiend!', 'ten minutes?', '\"sophie!\"', 'see?\"', '\"is there no one here?\"', 'everyone is in danger?', 'now!', '\"sophie?\"', 'why?\"', '\"who?\"', '\"and ... that?\"', 'or between china and japan?', '\"d\\'you know what that means?\"', '\"meaning?\"', \"'murder!'\", '\"what?\"', '91 \"here you are, holmes!', 'easy?', 'ha!', '\"when?\"', '\"me!\"', '\"no!\"', '\"what!', 'why?', '\"are you there?\"', 's142 to s152 ...', 'here ...', '\"no?\"', '\"what?', 'go!', '\"someone\\'s coming!\"', '\"ah!', '\"and what?\"', '\"no, i did not.', '\"what!', \"what do you want?'\", '\"fire?', 'what\\'s the address?\"', '\"look!', 'faces.15', \"'the a.b.c.?\", 'why?', 'trees!', '\"oui?\"', '\"what?\"', '\"what\\'s this?\"', 'nuts?', 'when will i reach the corner?', 'are you okay?\"', '\"you or he?\"', 'or of tokay?', 'exactly!', '\"what?\"', '\"no, sir, never!\"', '\"no?', '\"it is?\"'), ('যেতে দিন!', 'কীভাবে?', 'দেখ!', 'তিন...', 'টেনশন?', 'কখনও!', 'তাই নাকি?', 'হুম!', 'পিশাচ!', 'দশ মিনিট?', 'সোফি!', 'বুঝছ?', 'কেউ নেই এখানে?', 'সবাই বিপদে আছে...', 'এখনই!', 'সোফি?', 'কেন?', 'কে?', 'এবং...ওটা?', 'অথবা চীন এবং জাপান?', 'মানে জানেন?', 'মানে?', 'খুন!', 'কি?', 'হোমস, এই তো সেই নাম!', 'সহজ?', 'হা!', 'কখন?', 'আমি?', 'না!', 'কেন!', 'কেন?', 'আপনি কি ওখানে আছেন?', 'এস১৪২ থেকে এস১৫২...', 'এখানে...', 'না?', 'কী?', 'যাও!', 'কেউ আসছে!', 'হে ভগবান!', 'এবং কি?', 'না, চিনতাম না!', 'সে কী!', 'কি চাও তুমি?', 'আগুন?', 'ঠিকানাটা কি?', 'দেখ!', '১৫', 'এবিসি?', 'কেন?', 'গাছ!', 'উই?', 'কী?', 'এটা কি?', 'বাদাম?', 'কোনাটা কখন আসবে?', 'কেমন আছেন?', 'আপনি, নাকি উনি?', 'টোকে?', 'তাইতো!', 'কি?', 'না, একদম না!', 'না?', 'তাই নাকি?')]\n",
            "[('what the hell?!', '\"stop!\"', '\"meaning?\"', 'a code?', 'was she the final victim?', '\"sophie?', '\"hey!\"', '\"no!\"', '\"see that?!\"', '\"asleep?\"', 'what would have happened then?', '\"chief?\"', '\"what?', 'what happens now?\"', 'fireworks?', '\"nonsense!', 'the', '\"think!\"', 'no!', '\"you ...', 'was this life?', '\"brothers?\"', 'think!', '\"i don\\'t know!\"', '\"you okay?\"', 'what is the problem?', 'no?', 'me?\"', '\"no?', 'what is it?\"', '\"who the hell knows!', '\"well ...', 'no!', 'why 21 days?', '\"that being?\"', 'what is that?', 'wouldn\\'t you agree?\"', '\"what?', 'papa!', 'not a whit.', '\"aha!\"', '\"what?!\"', \"'who is it?'\", 'and why?', '\"bs.\"', '\"so?\"', 'hum!', '\"why do you imagine?\"', '\"is everything all right?\"', '\"are you okay?\"', '\"¡si, si!', 'no!', \"let's see...\", '\"and?\"', '\"you\\'re crying?\"', 'what was neveu thinking?', 'maybe both of them?', 'what would you do?\"', 'why me?', '\"how much?', 'how?\"', 'is everything okay?', 'those ...', 'no!'), ('এটা কি কিভাবে?!', 'থাম!', 'মানে?', 'এটা একটা কোড?', 'শেষ ভিকটিম কে?', 'সোফি?', 'শোন!', 'না!', 'এটা দেখছেন?!', 'ঘুম?', 'এতে কি হতো!', 'চীফ?', 'কী?', 'এখন তবে কি হবে?', 'আতশবাজি?', 'মোটেই না!', '২', 'ভাব!', 'না!', 'তুমি...?', 'এটাই কি জীবন?', 'ভাই ভাই?', 'ভাবো!', 'আমি জানি না!', 'তুমি ঠিক আছতো?', 'সেই জিনিসটা কি?', 'না?', 'আমি?', 'না?', 'কী সেটা?', 'কে জানে?', 'যাক...', 'না!', '২১দিন কেন?', 'সেটা কি?', 'কি এটা?', 'আপনি কি একমত নন?', 'কি?', 'পাপা!', 'একটুও না!', 'আহা!', 'কি?', 'কে ওখানে?', 'এবং কেন?', 'বিএস?', 'তো?', 'হুম!', 'আপনি কি ভাবছেন?', 'সবকিছু কি ঠিক আছে?', 'আপনি ঠিক আছেন তো?', 'সি, সি!', 'না!', 'দেখা যাক ...', 'কি?', 'তুমি কাদছ?', 'নেভু কি ভাবছে?', 'নাকি দুটাই?', 'আপনি কী বুঝেছেন?', 'আমি কেন?', 'কত?', 'কীভাবে?', 'সবকিছু ঠিক আছেতো?', 'সেই...', 'না!')]\n",
            "[('what would you do?', 'why ask more out of life?\"', 'know thyself', '\"what the...?\"', \"will you, babuji-sahib?'\", '\"really?\"', '\"who\\'s here?\"', 'father.', '\"what?\"', 'why?', '\"am i?', 'go.\"', 'this way, gentlemen, if you please.\"', '\"now!\"', '\"out!\"', '\"no!\"', \"'isn't that right, babuji?\", '\"this is good news, right?', 'no!', '\"what in the world am i looking at?\"', 'what do you want?', '\"in what way?\"', 'atoms!', '\"sub?\"', '\"how do you know this?\"', 'no!', 'why?', '\"coordinates?\"', '\"i\\'m listening.\"', '\"did you hear?', '\"possible, i admit ...', '\"how?\"', '\"who are you?!', '\"what?!\"', '\"what?', '\"...', 'there he is!\"', 'no!', '\"you!', \"'no!\", 'me, me, me ....\"', '\"wait!\"', 'hum!', '\"now?\"', '\"anna!', '\"no, no!', 'why?\"', \"is that correct?'\", '\"what?\"', 'today?', 'this is it?', 'why?\"', 'you\\'re okay?\"', '\"journalism?', \"'toba!\", '\"are you okay?\"', '\"and look at this!\"', 'hum!', 'forgive me, father ...', '\"no!\"', \"yes?'\", '\"what is it here?\"', 'was she hurt?', '\"now!\"'), ('আপনি কী কবে?', 'জীবন থেকে বেশি কিছু চাও কেন?', 'নিজেকে জানো', 'এটা কি....?', 'দেবেন না বাবুজি?', 'তাই নাকি?', 'এখানে কে আছে?', '.', 'কি?', 'কেন?', 'তাই?', 'গো!', 'এইদিকে আসুন!', 'এখনই!', 'আউট!', 'না!', 'তাই না বাবুজী?', 'এটা একটা সুসংবাদ, তাই না?', 'না!', 'এ আমি কী দেখছি?', 'আপনি কি চান?', 'কোন দিক থেকে?', 'গাছ!', 'সাব?', 'তুমি এটা কীভাবে জানো?', 'না!', 'কেন?', 'ঠিকানা?', 'আমি শুনছি?', 'তুমি কি শুনেছো?', 'আমি মানছি...', 'কিভাবে?', 'তুমি কে??!', 'কি?!', 'কী?', '...', 'ওই আসছে!', 'না!', 'আপনি!', 'না!', \"'আমি, আমি, আমি...!\", 'থামুন!', 'হুম?', 'এখনই?', 'আনা!', 'না, না!', 'কেন?', 'ঠিক কি না?', 'কি?', '...আজ?', 'এটা কি?', 'কেন?', 'ঠিক আছেনতো আপনি?', 'সাংবাদিকতা?', 'তৌবা!', 'তুমি ঠিক আছো?', 'এদিকে দেখুন!', 'হুম!', 'বাবা...', 'না!', 'তাই না?', 'এখানে কী?', 'সে কি আহত?', 'এখনই!')]\n",
            "[('\"why not?!\"', '\"no!\"', '\"tut!', '\"everyone comfy?\"', '\"deliberate?', \"how ...'\", '\"no?\"', \"he'll never let me out of here!\", 'in the gardens?', '\"do you know someone?\"', 'why?\"', 'look!\"', '\"i\\'m sorry?\"', '\"jonas?\"', 'how?', 'they', '\"are you really so naive?', '\"yes!', 'is he okay?!\"', '\"good.\"', 'why?', '\"amen!', 'ten days?', 'exactly!', '\"what?\"', '\"mon dieu, non!\"', 'no ...', 'me!', 'no!', 'a key?', 'modern time', '...', '\"p.s.?\"', 'you know what?', \"'policy, you know!\", 'think!', '\"to-day?\"', \"'listen!'\", 'god.\"', '\"did you ever trouble to see who called for them?\"', '\"what is this?\"', '\"what?\"', \"there's a lot else!\", 'no ...', 'what?', '\"what?!\"', 'why?', 'no!', '\"how the hell would you know that?!\"', '\"oh?\"', '\"today?!', '\"jonas?\"', '\"then who are you?\"', 'that:', '\"look!\"', '\"what is it?\"', 'or between china and japan?', 'that\\'s the ring.\"', '\"oh?', 'who knows.', '\"me?\"', 'are you okay?\"', '\"out, son!', 'maybe three.'), ('কেন?', 'আসেননি!', 'ছোঃ!', 'সবাই ঠিক আছেন তো?', 'পৈশাচিকতা?', 'কি ভাবে...', 'না?', 'সে আমাকে কখনও এখান থেকে যেতে দেবে না!', 'বাগানে?', 'তুমি এমন কাউকে চেনো?', 'কেন?', 'দেখ!', 'দুঃখিত?', 'জোনাস?', 'কিভাবে?', 'ধাপ', 'তুমি কি এতই বোকা?', 'হা!', 'সে কেমন আছে?!', 'গুড!', 'কেন?', 'তাই হোক!', 'দশ দিন?', 'ঠিক তাই!', 'কি?', 'মদিউ, নো!', 'না...', 'আমি!', 'না!', 'একটা চাবি?', 'আধুনিক যুগ', '...', 'পি এস?', 'একটা কথা জান?', 'নীতি, তুমি জান!', 'ভাববা!', 'আজ?', 'শোন!', 'গড!', 'নোট নিতে কে আসে কখনো দেখতে যাননি?', 'এটা কি?', 'কি?', 'অনেক কিছু জানতে চাই!', 'না...', 'কি?', 'কি?', 'কেন?', 'না!', 'তুমি এসব কিভাবে জানো?', 'ওহ?', 'আজই?!', 'জোনাস?', 'কে তুমি?', '...', 'দেখো!', 'এটা কি?', 'অথবা চীন এবং জাপান?', 'এই সেই আংটি!', 'তাই নাকি?', 'কে জানে!', 'আমি?', 'তুমি কি ঠিক আছে?', 'আউট!', 'নাকি তিনটা?')]\n",
            "[(\"why me?'\", 'could it be?', 'oh come on!', '\"who\\'s winning?\"', \"'why?'\", 'shit!', '\"there!\"', 'what?', 'how?', 'what is he thinking?', \"'me?\", '\"the pope is here.\"', 'how is he?\"', 'ha!', '\"oh?\"', '\"stop!', '...', '\"is somebody there?\"', '\"no, i won\\'t,\" he replied.', '\"no?\"', 'enjoyed?', '\"who is the printer?\"', 'are you okay?\"', '\"what?\"', 'the floating ...', '\"um ...', '\"no?', '\"and how do you reply?\"', 'pax atomica', 'no!', \"'land!\", '\"no bodies?', '\"now!\"', '\"no?\"', '\"oh, indeed!', '\"how so?\"', 'who is an optimist?', '\"and that includes ... you?\"', 'no!', \"'i?\", 'violence?', \"and why shouldn't it?\", '\"three minutes, father!', 'why?', 'but who?', 'what was it?', '\"no!', 'or disproved as myth?', '\"no!', '\"who knows?\"', '\"no?\"', 'the p.s.?', 'amin!', 'you see it all exactly as i found it.\"', '\"look out!\"', 'in the office.\"', '\"look!', '\"you know nothing!\"', 'milk or sugar?\"', '\"no!\"', '\" ...', '\"why?\"', 'why?', '\"why?\"'), ('আমি কেন?', '২৪) এটাই কি সেটা?', 'ও, কাম অন!', 'কে জিতছে?', \"কেন?'\", 'উফ!', 'এইতো!', 'কি?', 'কীভাবে?', 'কি ভাবছে সে?', 'আমি?', 'পোপ এখানে থাকেন?', 'কেমন আছে সে?', 'হা!', 'ওহ?', 'থামুন!', '.', 'ওখানে কি কেউ আছেন?', \"'না তা হবে না!'\", 'দেখেননি?', 'উপভোগ?', 'কে ছেপেছে?', 'তুমি ঠিক আছে তো?', 'কি?', 'সেই ভেসে থাকা...', 'উম...', 'না?', 'কী জবাব দেন তখন?', 'আণবিক সমঝোতা', 'না!', 'ডাঙা!\"', 'কোন শব নেই?', 'এখনই!', 'না?', 'তাই নাকি!', 'কিভাবে?', 'আশাবাদী কি?', 'তোমাকে...সহ?', 'না!', 'আমি?', 'খুনোখুনি?', 'এবং আসবে নাই বা কেন?', 'তিন মিনিট!', 'কেন?', 'কে?', 'এটা কী?', 'না!', 'বা মিথ হিসাবে নাকচ হবে?', 'না!', 'কে জানে?', 'না?', 'পি,এস?', 'আমিন!', 'ঠিক যেভাবে আপনি দেখেছেন, আমিও দেখেছি সেইভাবে!', 'সাবধান!', 'অফিসে আছে?', 'দেখ!', 'তুমি কিছুই জানো না!', 'দুধ, না চিনি?', 'না!', '...', 'কেন?', 'কেন?', 'কেন?')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "criterian = nn.CrossEntropyLoss(ignore_index=bangla_to_index[PADDING_TOKEN],\n",
        "                                reduction='none')\n",
        "\n",
        "# When computing the loss, we are ignoring cases when the label is the padding token\n",
        "for params in transformer.parameters():\n",
        "    if params.dim() > 1:\n",
        "        nn.init.xavier_uniform_(params)\n",
        "\n",
        "optim = torch.optim.Adam(transformer.parameters(), lr=1e-4)\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "metadata": {
        "id": "X9RP7DiFwYrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NEG_INFTY = -1e9\n",
        "\n",
        "def create_masks(eng_batch, bn_batch):\n",
        "    num_sentences = len(eng_batch)\n",
        "    look_ahead_mask = torch.full([max_sequence_length, max_sequence_length] , True)\n",
        "    look_ahead_mask = torch.triu(look_ahead_mask, diagonal=1)\n",
        "    encoder_padding_mask = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
        "    decoder_padding_mask_self_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
        "    decoder_padding_mask_cross_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
        "\n",
        "    for idx in range(num_sentences):\n",
        "      eng_sentence_length, bn_sentence_length = len(eng_batch[idx]), len(bn_batch[idx])\n",
        "      eng_chars_to_padding_mask = np.arange(eng_sentence_length + 1, max_sequence_length)\n",
        "      bn_chars_to_padding_mask = np.arange(bn_sentence_length + 1, max_sequence_length)\n",
        "      encoder_padding_mask[idx, :, eng_chars_to_padding_mask] = True\n",
        "      encoder_padding_mask[idx, eng_chars_to_padding_mask, :] = True\n",
        "      decoder_padding_mask_self_attention[idx, :, bn_chars_to_padding_mask] = True\n",
        "      decoder_padding_mask_self_attention[idx, bn_chars_to_padding_mask, :] = True\n",
        "      decoder_padding_mask_cross_attention[idx, :, eng_chars_to_padding_mask] = True\n",
        "      decoder_padding_mask_cross_attention[idx, bn_chars_to_padding_mask, :] = True\n",
        "\n",
        "    encoder_self_attention_mask = torch.where(encoder_padding_mask, NEG_INFTY, 0)\n",
        "    decoder_self_attention_mask =  torch.where(look_ahead_mask + decoder_padding_mask_self_attention, NEG_INFTY, 0)\n",
        "    decoder_cross_attention_mask = torch.where(decoder_padding_mask_cross_attention, NEG_INFTY, 0)\n",
        "    return encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask"
      ],
      "metadata": {
        "id": "FB-v12Dgwcm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_device():\n",
        "    return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "metadata": {
        "id": "UC0LcsRJfByN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.train()\n",
        "transformer.to(device)\n",
        "total_loss = 0\n",
        "num_epochs = 30\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch}\")\n",
        "    iterator = iter(train_loader)\n",
        "    for batch_num, batch in enumerate(iterator):\n",
        "        transformer.train()\n",
        "        eng_batch, bn_batch = batch\n",
        "        encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(eng_batch, bn_batch)\n",
        "        optim.zero_grad()\n",
        "        bn_predictions = transformer(eng_batch,\n",
        "                                     bn_batch,\n",
        "                                     encoder_self_attention_mask.to(device),\n",
        "                                     decoder_self_attention_mask.to(device),\n",
        "                                     decoder_cross_attention_mask.to(device),\n",
        "                                     enc_start_token=False,\n",
        "                                     enc_end_token=False,\n",
        "                                     dec_start_token=True,\n",
        "                                     dec_end_token=True)\n",
        "        labels = transformer.decoder.sentence_embedding.batch_tokenize(bn_batch, start_token=False, end_token=True)\n",
        "        loss = criterian(\n",
        "            bn_predictions.view(-1, bn_vocab_size).to(device),\n",
        "            labels.view(-1).to(device)\n",
        "        ).to(device)\n",
        "        valid_indicies = torch.where(labels.view(-1) == bangla_to_index[PADDING_TOKEN], False, True)\n",
        "        loss = loss.sum() / valid_indicies.sum()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        #train_losses.append(loss.item())\n",
        "        if batch_num % 100 == 0:\n",
        "            print(f\"Iteration {batch_num} : {loss.item()}\")\n",
        "            print(f\"English: {eng_batch[0]}\")\n",
        "            print(f\"Bangla Translation: {bn_batch[0]}\")\n",
        "            bn_sentence_predicted = torch.argmax(bn_predictions[0], axis=1)\n",
        "            predicted_sentence = \"\"\n",
        "            for idx in bn_sentence_predicted:\n",
        "              if idx == bangla_to_index[END_TOKEN]:\n",
        "                break\n",
        "              predicted_sentence += index_to_bangla[idx.item()]\n",
        "            print(f\"Bangla Prediction: {predicted_sentence}\")\n",
        "\n",
        "\n",
        "            transformer.eval()\n",
        "            bn_sentence = (\"\",)\n",
        "            eng_sentence = (\"should we go to the mall?\",)\n",
        "            for word_counter in range(max_sequence_length):\n",
        "                encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask= create_masks(eng_sentence, bn_sentence)\n",
        "                predictions = transformer(eng_sentence,\n",
        "                                          bn_sentence,\n",
        "                                          encoder_self_attention_mask.to(device),\n",
        "                                          decoder_self_attention_mask.to(device),\n",
        "                                          decoder_cross_attention_mask.to(device),\n",
        "                                          enc_start_token=False,\n",
        "                                          enc_end_token=False,\n",
        "                                          dec_start_token=True,\n",
        "                                          dec_end_token=False)\n",
        "                next_token_prob_distribution = predictions[0][word_counter] # not actual probs\n",
        "                next_token_index = torch.argmax(next_token_prob_distribution).item()\n",
        "                next_token = index_to_bangla[next_token_index]\n",
        "                bn_sentence = (bn_sentence[0] + next_token, )\n",
        "                if next_token == END_TOKEN:\n",
        "                  break\n",
        "\n",
        "            print(f\"Evaluation translation (should we go to the mall?) : {bn_sentence}\")\n",
        "            print(\"-------------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6sG5zHjwguH",
        "outputId": "f6c3b0eb-d3e5-423e-f6f5-d7e61e76a90f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0\n",
            "Iteration 0 : 5.764863014221191\n",
            "English: let me pass.\"\n",
            "Bangla Translation: যেতে দিন!\n",
            "Bangla Prediction: |>>চ্ছ্বচ্ছ্বপ>চ্ছ্বচ্ছ্ব>জ্ঞচ্ছ্বন্ত্রন্ত্রন্ত্র৬দ্দজ্ঞন্ত্রজ্ঞজ্ঞমন্ত্রজ্ঞ%ন্ত্রজ্ঞজ্ঞন্ত্রচ্ছ্ব|ন্ত্রন্ত্রজ্ঞন্ত্রন্ত্রন্ত্রংউন্ত্রদ্দজ্ঞন্ত্রজ্ঞজ্ঞন্বন্ত্রধজ্ঞজ্ঞন্ত্রজ্ঞন্বস্তজ্ঞন্ত্রন্ত্রন্ত্রন্ত্রন্ত্রদ্দন্ত্রচ্ছ্বজ্ঞজ্ঞজ্ঞচ্ছ্বস্তজ্ঞদ্দস্তচ্ছ্ব%ন্বন্ত্রমন্ত্রন্ত্রজ্ঞমচ্ছ্বজ্ঞদ্দজ্ঞন্তন্বন্ত্রজ্ঞচ্ছ্বজ্ঞন্বন্ত্রজ্ঞ'স্তজ্ঞন্ত্রস্তস্তজ্ঞ+জ্ঞন্ত্রদ্দজ্ঞ>|ন্ত্রদ্দ|ন্ত্রজ্ঞস্তচ্ছ্বস্তজ্ঞজ্ঞন্ত্রন্ত্রচ্ছ্বচ্ছ্বন্ত্রন্বজ্ঞজ্ঞন্ত্রন্ত্রন্ত্রজ্ঞজ্ঞজ্ঞউজ্ঞন্ত্রচ্ছ্বজ্ঞজ্ঞস্তন্ত্রন্ত্রজ্ঞজ্ঞজ্ঞন্ত্রন্ত্র+জ্ঞজ্ঞজ্ঞদ্দন্ত্রজ্ঞজ্ঞ%স্তস্তজ্ঞজ্ঞজ্ঞন্ত্র%ন্ত্রন্ত্রন্ত্রন্ত্রন্বন্ত্রচ্ছ্বন্ত্রন্বন্ত্রন্ত্রস্তন্ত্রজ্ঞজ্ঞন্বদ্দজ্ঞচ্ছ্বন্ত্রদ্দন্ত্রজ্ঞজ্ঞন্ত্রন্ত্রজ্ঞন্ত্রচ্ছ্বন্ত্রজ্ঞজ্ঞদ্দক্রজ্ঞন্ত্রন্ত্রউন্ত্র|(মন্ত্রন্ত্রন্ত্রস্ত|ন্ত্রজ্ঞন্বদ্দমন্ত্রজ্ঞন্ত্রন্ত্রচ্ছ্বস্তন্ত্রজ্ঞদ্দজ্ঞ$চ্ছ্বজ্ঞজ্ঞজ্ঞদ্দজ্ঞজ্ঞন্ত্রজ্ঞন্ত্রন্ত্র$চ্ছ্বস্তন্বজ্ঞন্ত্রন্ত্রজ্ঞচ্ছ্বন্ত্রমন্ত্র$ন্ত্রস্ত\n",
            "Evaluation translation (should we go to the mall?) : ('<END>',)\n",
            "-------------------------------------------\n",
            "Epoch 1\n",
            "Iteration 0 : 3.4523322582244873\n",
            "English: let me pass.\"\n",
            "Bangla Translation: যেতে দিন!\n",
            "Bangla Prediction: \n",
            "Evaluation translation (should we go to the mall?) : ('<END>',)\n",
            "-------------------------------------------\n",
            "Epoch 2\n",
            "Iteration 0 : 3.510493755340576\n",
            "English: let me pass.\"\n",
            "Bangla Translation: যেতে দিন!\n",
            "Bangla Prediction: \n",
            "Evaluation translation (should we go to the mall?) : ('<END>',)\n",
            "-------------------------------------------\n",
            "Epoch 3\n",
            "Iteration 0 : 3.460770845413208\n",
            "English: let me pass.\"\n",
            "Bangla Translation: যেতে দিন!\n",
            "Bangla Prediction: \n",
            "Evaluation translation (should we go to the mall?) : ('<END>',)\n",
            "-------------------------------------------\n",
            "Epoch 4\n",
            "Iteration 0 : 3.411424160003662\n",
            "English: let me pass.\"\n",
            "Bangla Translation: যেতে দিন!\n",
            "Bangla Prediction: \n",
            "Evaluation translation (should we go to the mall?) : ('<END>',)\n",
            "-------------------------------------------\n",
            "Epoch 5\n",
            "Iteration 0 : 3.3992700576782227\n",
            "English: let me pass.\"\n",
            "Bangla Translation: যেতে দিন!\n",
            "Bangla Prediction: \n",
            "Evaluation translation (should we go to the mall?) : ('<END>',)\n",
            "-------------------------------------------\n",
            "Epoch 6\n",
            "Iteration 0 : 3.36205792427063\n",
            "English: let me pass.\"\n",
            "Bangla Translation: যেতে দিন!\n",
            "Bangla Prediction: কাা\n",
            "Evaluation translation (should we go to the mall?) : ('াাা <END>',)\n",
            "-------------------------------------------\n",
            "Epoch 7\n",
            "Iteration 0 : 3.311143636703491\n",
            "English: let me pass.\"\n",
            "Bangla Translation: যেতে দিন!\n",
            "Bangla Prediction: ককক\n",
            "Evaluation translation (should we go to the mall?) : ('এএক<END>',)\n",
            "-------------------------------------------\n",
            "Epoch 8\n",
            "Iteration 0 : 3.240612268447876\n",
            "English: let me pass.\"\n",
            "Bangla Translation: যেতে দিন!\n",
            "Bangla Prediction: এতকক\n",
            "Evaluation translation (should we go to the mall?) : ('এএ         <END>',)\n",
            "-------------------------------------------\n",
            "Epoch 9\n",
            "Iteration 0 : 3.2581233978271484\n",
            "English: let me pass.\"\n",
            "Bangla Translation: যেতে দিন!\n",
            "Bangla Prediction: কককন\n",
            "Evaluation translation (should we go to the mall?) : ('এএকককক<END>',)\n",
            "-------------------------------------------\n",
            "Epoch 10\n",
            "Iteration 0 : 3.230841875076294\n",
            "English: let me pass.\"\n",
            "Bangla Translation: যেতে দিন!\n",
            "Bangla Prediction: কক  \n",
            "Evaluation translation (should we go to the mall?) : ('এম      <END>',)\n",
            "-------------------------------------------\n",
            "Epoch 11\n",
            "Iteration 0 : 3.2076573371887207\n",
            "English: let me pass.\"\n",
            "Bangla Translation: যেতে দিন!\n",
            "Bangla Prediction: কতি.\n",
            "Evaluation translation (should we go to the mall?) : ('এমিিক<END>',)\n",
            "-------------------------------------------\n",
            "Epoch 12\n",
            "Iteration 0 : 3.2523980140686035\n",
            "English: let me pass.\"\n",
            "Bangla Translation: যেতে দিন!\n",
            "Bangla Prediction: একে      \n",
            "Evaluation translation (should we go to the mall?) : ('এতা               <END>',)\n",
            "-------------------------------------------\n",
            "Epoch 13\n",
            "Iteration 0 : 3.1153173446655273\n",
            "English: let me pass.\"\n",
            "Bangla Translation: যেতে দিন!\n",
            "Bangla Prediction: এতা  \n",
            "Evaluation translation (should we go to the mall?) : ('এমা      <END>',)\n",
            "-------------------------------------------\n",
            "Epoch 14\n",
            "Iteration 0 : 3.067534923553467\n",
            "English: let me pass.\"\n",
            "Bangla Translation: যেতে দিন!\n",
            "Bangla Prediction: এাা   \n",
            "Evaluation translation (should we go to the mall?) : ('এুা        <END>',)\n",
            "-------------------------------------------\n",
            "Epoch 15\n",
            "Iteration 0 : 3.107267141342163\n",
            "English: let me pass.\"\n",
            "Bangla Translation: যেতে দিন!\n",
            "Bangla Prediction: কো  \n",
            "Evaluation translation (should we go to the mall?) : ('এমা      <END>',)\n",
            "-------------------------------------------\n",
            "Epoch 16\n",
            "Iteration 0 : 3.0465211868286133\n",
            "English: let me pass.\"\n",
            "Bangla Translation: যেতে দিন!\n",
            "Bangla Prediction: কমি    \n",
            "Evaluation translation (should we go to the mall?) : ('এুা        <END>',)\n",
            "-------------------------------------------\n",
            "Epoch 17\n",
            "Iteration 0 : 2.939237356185913\n",
            "English: let me pass.\"\n",
            "Bangla Translation: যেতে দিন!\n",
            "Bangla Prediction: এুা   ি\n",
            "Evaluation translation (should we go to the mall?) : ('এুম          <END>',)\n",
            "-------------------------------------------\n",
            "Epoch 18\n",
            "Iteration 0 : 2.9284071922302246\n",
            "English: let me pass.\"\n",
            "Bangla Translation: যেতে দিন!\n",
            "Bangla Prediction: এমা  ি..?\n",
            "Evaluation translation (should we go to the mall?) : ('এমম           া াননননননন<END>',)\n",
            "-------------------------------------------\n",
            "Epoch 19\n",
            "Iteration 0 : 2.8693809509277344\n",
            "English: let me pass.\"\n",
            "Bangla Translation: যেতে দিন!\n",
            "Bangla Prediction: কম    ...\n",
            "Evaluation translation (should we go to the mall?) : ('এুম             া াাাাাাাাাাাাােোাা.<END>',)\n",
            "-------------------------------------------\n",
            "Epoch 20\n",
            "Iteration 0 : 3.178969621658325\n",
            "English: let me pass.\"\n",
            "Bangla Translation: যেতে দিন!\n",
            "Bangla Prediction: এমন\n",
            "Evaluation translation (should we go to the mall?) : ('এমম                                                                             ন                         ন                    ে              ননন ন ননননন ননা ে নননন  ননননননননননন   ননননন ননননন ন নন    নেননন নননেনা ে েেেে ন নেনে     ননননন  ননননননন   নন',)\n",
            "-------------------------------------------\n",
            "Epoch 21\n",
            "Iteration 0 : 2.9035093784332275\n",
            "English: let me pass.\"\n",
            "Bangla Translation: যেতে দিন!\n",
            "Bangla Prediction: এত    কক \n",
            "Evaluation translation (should we go to the mall?) : ('এুম         াাাাাাাাাাাাাা<END>',)\n",
            "-------------------------------------------\n",
            "Epoch 22\n",
            "Iteration 0 : 2.8171212673187256\n",
            "English: let me pass.\"\n",
            "Bangla Translation: যেতে দিন!\n",
            "Bangla Prediction: এাা    .া\n",
            "Evaluation translation (should we go to the mall?) : ('এপ                 া   া া                   ে ে<END>',)\n",
            "-------------------------------------------\n",
            "Epoch 23\n",
            "Iteration 0 : 2.711737632751465\n",
            "English: let me pass.\"\n",
            "Bangla Translation: যেতে দিন!\n",
            "Bangla Prediction: সুি  নিনন\n",
            "Evaluation translation (should we go to the mall?) : ('এুা  ননননাননননননননননননননননননননননননননননননননননননানানানানাাাাাাাাাাাাাাানানানাাাাাাাাাাাাাাাাাাাাাাাাাাাাাাাাাাাাাাাাাাাাাাাাাাাাাাােোাাাাাাাাাাানানাাাাাাাাাাাাাাাাাাাাানানাে.<END>',)\n",
            "-------------------------------------------\n",
            "Epoch 24\n",
            "Iteration 0 : 2.765549421310425\n",
            "English: let me pass.\"\n",
            "Bangla Translation: যেতে দিন!\n",
            "Bangla Prediction: এা   ক...\n",
            "Evaluation translation (should we go to the mall?) : ('এুমি কে    ে ে ে নে ে ে েে ে  ে ে ে ে ে ে ে ে ে ে ?<END>',)\n",
            "-------------------------------------------\n",
            "Epoch 25\n",
            "Iteration 0 : 2.6810688972473145\n",
            "English: let me pass.\"\n",
            "Bangla Translation: যেতে দিন!\n",
            "Bangla Prediction: এুন  কি?া\n",
            "Evaluation translation (should we go to the mall?) : ('এুি আকি আ আআআে ননননন?<END>',)\n",
            "-------------------------------------------\n",
            "Epoch 26\n",
            "Iteration 0 : 2.523069381713867\n",
            "English: let me pass.\"\n",
            "Bangla Translation: যেতে দিন!\n",
            "Bangla Prediction: এানা নে!.\n",
            "Evaluation translation (should we go to the mall?) : ('এতনি এা এএসে এনে এনননননে নে নে নে নে সে নে.<END>',)\n",
            "-------------------------------------------\n",
            "Epoch 27\n",
            "Iteration 0 : 2.437220335006714\n",
            "English: let me pass.\"\n",
            "Bangla Translation: যেতে দিন!\n",
            "Bangla Prediction: এুন! নি!!\n",
            "Evaluation translation (should we go to the mall?) : ('এপনি কি একা এসে এনান?<END>',)\n",
            "-------------------------------------------\n",
            "Epoch 28\n",
            "Iteration 0 : 2.3519134521484375\n",
            "English: let me pass.\"\n",
            "Bangla Translation: যেতে দিন!\n",
            "Bangla Prediction: এুনা কি?ে\n",
            "Evaluation translation (should we go to the mall?) : ('এখা এনি এনা এনে এনান?<END>',)\n",
            "-------------------------------------------\n",
            "Epoch 29\n",
            "Iteration 0 : 2.2622601985931396\n",
            "English: let me pass.\"\n",
            "Bangla Translation: যেতে দিন!\n",
            "Bangla Prediction: তিন  নন !\n",
            "Evaluation translation (should we go to the mall?) : ('এটি এনি এনে এছে এনা?<END>',)\n",
            "-------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.eval()\n",
        "def translate(eng_sentence):\n",
        "  eng_sentence = (eng_sentence,)\n",
        "  bn_sentence = (\"\",)\n",
        "  for word_counter in range(max_sequence_length):\n",
        "    encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask= create_masks(eng_sentence, bn_sentence)\n",
        "    predictions = transformer(eng_sentence,\n",
        "                              bn_sentence,\n",
        "                              encoder_self_attention_mask.to(device),\n",
        "                              decoder_self_attention_mask.to(device),\n",
        "                              decoder_cross_attention_mask.to(device),\n",
        "                              enc_start_token=False,\n",
        "                              enc_end_token=False,\n",
        "                              dec_start_token=True,\n",
        "                              dec_end_token=False)\n",
        "    next_token_prob_distribution = predictions[0][word_counter]\n",
        "    next_token_index = torch.argmax(next_token_prob_distribution).item()\n",
        "    next_token = index_to_bangla[next_token_index]\n",
        "    bn_sentence = (bn_sentence[0] + next_token, )\n",
        "    if next_token == END_TOKEN:\n",
        "      break\n",
        "  return bn_sentence[0]"
      ],
      "metadata": {
        "id": "pfQaIfGKwoeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translation = translate(\"how are you?\")\n",
        "print(translation)"
      ],
      "metadata": {
        "id": "LvR6uKv6w2S7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "827540ff-19f0-40bd-e294-e8114e00ae4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "এটাই কেনে?<END>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translation = translate(\"let me go\")\n",
        "print(translation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWmpYQvMCoEb",
        "outputId": "340182fe-9025-43ce-9c2f-cdf72308043c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "কেই কে?<END>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translation = translate(\"do not come here\")\n",
        "print(translation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15yBgNB8Shui",
        "outputId": "9d243d0f-baa9-4609-ecfe-c3e79ac4ca5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "এটাই কেনে?<END>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translation = translate(\"i will go to the market\")\n",
        "print(translation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUxQo9shSoFH",
        "outputId": "6c7518e5-c6fc-4ab0-b26d-d9281e2f78ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "এটানে কেনেনে?<END>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translation = translate(\"where are you going?\")\n",
        "print(translation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tv2R8__eSsy6",
        "outputId": "530e1847-278e-428f-adb3-a3e7641f7fc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "এটানে কেনে?<END>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translation = translate(\"let me pass\")\n",
        "print(translation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZ2qnbmBWgM0",
        "outputId": "65b329d8-e75e-4d83-f2f0-d111b4273193"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "এটাই কেনে?<END>\n"
          ]
        }
      ]
    }
  ]
}